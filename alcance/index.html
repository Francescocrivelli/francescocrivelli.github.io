<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Robotic Feeding Assistant for Quadriplegic Individuals in LATAM">
  <meta name="keywords" content="Robotic Feeding Assistant for Quadriplegic Individuals in LATAM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EL-ALCANCE</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="../nerfies_style/static/css/bulma.min.css">
  <link rel="stylesheet" href="../nerfies_style/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../nerfies_style/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../nerfies_style/static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../nerfies_style/static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../nerfies_style/static/js/fontawesome.all.min.js"></script>
  <script src="../nerfies_style/static/js/bulma-carousel.min.js"></script>
  <script src="../nerfies_style/static/js/bulma-slider.min.js"></script>
  <script src="../nerfies_style/static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.francescocrivelli.com/projects.html">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EL-ALCANCE</h1>
          <h2 class="subtitle is-3 publication-title">Robotic Feeding Assistant for Quadriplegic Individuals in LATAM</h2>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://www.francescocrivelli.com">Francesco Crivelli</a><sup>1</sup>,</span>
            <span class="author-block">Bryan Sow<sup>1</sup>,</span>
            <span class="author-block">Claire Beaudin<sup>1</sup>,</span>
            <span class="author-block">Boris Tomov<sup>1</sup></span>
            <span class="author-block">Professor Roberto Horrowitz<sup>2</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California Berkeley, <a href="https://tukuypaj.org">Tukuypaj Chile</a>, <a href="https://recursivepioneer.org">Recursive at Berkeley</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link.
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>report</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Francescocrivelli/le-alcance"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/francescocrivelli/carrot_eating"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
              <!-- Model Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/francescocrivelli/carrot_feeder_model_ACT_Policy"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-brain"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span>
              
            </div>
          </div>
        </div>
    </div>
  </div>
</section>

<section class="hero teaser">
    <div class="container">
      <div class="hero-body" style="padding: 0;">
        <figure>
          <img src="./media/robot_feeding_francesco.gif" 
               alt="Hardware SO-ARM" 
               style="width: 70%;
               border-radius: 8px;
               display: block;
               margin: 0 auto;
               box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <figcaption style="text-align: center; margin-top: 10px;">
            Figure 1: Robot feeding demonstration with Francesco
          </figcaption>
        </figure>
      </div>
    </div>
  </section>

<section class="hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content">
        <h2 class="title is-3">Abstract</h2>
        <p class="has-text-justified">
            We present a novel low-cost assistive robotic feeding system developed in collaboration between UC Berkeley's <a href="https://recursivepioneer.org/">Recursive Pioneers</a> and <a href="https://www.tukuypaj.org/">Tukuypaj</a>, a Chilean non-profit organization serving individuals with severe motor disabilities. We learnt that the organization is often short-staffed in having available personnel to feed their quadriplegic community during mealtimes. Therefore, our primary project objective was to build a robotic system that can help reach for food and feed quadriplegic beneficiaries. We aspired to create a robotic system that grants the quadriplegic community greater self-sufficiency, while devising a solution that can be replicated at scale. Our work demonstrates that high-performance feeding assistance can be achieved using affordable hardware - 
            specifically the SO-100 robotic arm platform ($110 per arm), which we've enhanced through custom modifications and advanced control algorithms. Prior to system development, we conducted extensive interviews with <a href="https://www.tukuypaj.orgn/">Tukuypaj</a>'s beneficiaries and caregivers, gathering crucial insights about the challenges and requirements of assisted feeding,
            which directly informed our technical specifications and design choices. Our technical implementation leverages the LeRobot framework for data collection and utilizes an Adversarial Control Transformer (ACT) architecture trained on demonstration data, exploring the feasibility of combining imitation learning with affordable hardware to
            create accessible robotic solutions. Initial development has focused on establishing reliable control mechanisms and movement patterns using the LeRobot framework's sophisticated tools for robot control and training, with an emphasis on system stability and movement precision while maintaining the low-cost advantage of the SO-100 platform.
            Through systematic testing in controlled environments, we are laying the groundwork for future real-world applications, representing an important step toward making assistive robotics technology more accessible through cost-effective solutions. By combining affordable hardware with sophisticated control frameworks, we demonstrate the potential
            for bringing advanced robotics assistance to resource-constrained settings, setting the stage for future field testing and deployment in therapeutic environments across <a href="https://www.tukuypaj.org/">Tukuypaj</a>'s centers and similar care facilities worldwide.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
    <div class="container">
      <div class="hero-body" style="padding: 0;">
        <figure>
          <video width="50%" autoplay loop muted playsinline style="border-radius: 8px; display: block; margin: 0 auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            <source src="./media/all_team_getting_setup_working_time_laps_compressed.mov" type="video/mp4">
            <source src="./media/all_team_getting_setup_working_time_laps_compressed.webm" type="video/webm">
            <source src="./media/all_team_getting_setup_working_time_laps_compressed.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption style="text-align: center; margin-top: 10px;">
            Figure 2: Time lapse of team setup and development process
          </figcaption>
        </figure>
      </div>
    </div>
  </section>





<!-- <section class="hero teaser"></section>
    <div class="container">
      <div class="hero-body" style="padding: 0;">
        <img src="./media/luis.gif" 
             alt="Hardware SO-ARM" 
             style="width: 30%;
             border-radius: 8px;
             display: block;
             margin: 0 auto;
             box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      </div>
    </div>
  </section> -->

  <section class="hero">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="content">
          <details>
            <summary class="title is-3" style="cursor: pointer; margin-bottom: 1rem;">Design Ideology</summary>
            <div class="content has-text-justified">
  
              <h3>Design Ideology: Low-Cost and Scalable Robotics to Tackle Real-World Problems</h3>
  
              <p>The first steps of our project involved going through the design process, conducting interviews to gather data on the problem and gain insight into the target users that we are designing for. There were two key insights that we learnt about the beneficiaries during the interview process:</p>
              <ul>
                <li>Most of the quadriplegic interviewees are non-vocal, and heavily rely on facial expressions to communicate. For instance, when a caretaker picks up a certain food item, they would smile to indicate if they would like the food item or not.</li>
                <li>We learnt that their main diet consists of chopped up food that requires the use of a spoon. One caretaker was kindly willing to help take a video of their feeding process. Studying the video, we learnt the path motions that our robotic arm needed to replicate to accomplish such a task.</li>
              </ul>
  
              <p>The desired functionality of our system involves the creation of path trajectories for the robot system to bring the food to the beneficiary's mouth, while enabling the beneficiary to enact the movement on his/her own. Our main design criteria involves creating a simplified system that is easy to implement, ensuring robustness in reproducibility and reliability, all while ensuring that the system does not deviate too far from their usual feeding routine to ensure a sense of familiarity and easy adoption. The design that we chose is in alignment with this design philosophy.</p>
  
              <p>We first considered constrained situations where we have fixed types of food and fixed relative positions between the food item and the beneficiary. We knew that when our robotic system would be deployed, it would need to perform its function in various environments and situations. Furthermore, we also wanted direct human input over the path trajectories of the robot arm, such that the caretakers can directly dictate how the robot brings the food to the beneficiary and personalize the trajectories for each of them. To fulfill these conditions, we thought it was best for our system to employ a path planning setup where we had two robot arms – one which allowed a caretaker to manually create the path trajectory, and the other arm replicating and storing the motion. We knew that creating and obtaining hardware for two arms was much more difficult than for one, but we felt that this design choice would allow for the path planning element of the project to be easier to implement, and allow caretakers greater influence over the movement of the robot arms as they interact with the beneficiaries in the physical world.</p>
  
              <p>Secondly, we also wanted to create a bridge where the beneficiaries can directly communicate with the robot system. Learning that they mainly communicate through facial expressions, we knew that incorporating computer vision to read their expressions was crucial in our design setup. It was hard to decide upon a mechanism for communication as in reality we learnt that every beneficiary has a different way of communicating their needs through facial expressions (some smile while others chuff), hence we thought it was easiest to standardize the manner of communication. We decided upon having the robot system react to the beneficiaries opening their mouths to bring the food to them.</p>
  
              <p>In tandem, the overall flow of the use of the robot system was intended as follows: the caretakers can manually create the path trajectories, and the beneficiaries can execute the movement of the arms along these trajectories by opening their mouths.</p>
  
              <p>We also aspired to incorporate a second tier to our design philosophy, where we considered dynamic situations involving different types of food, positions of food relative to the beneficiary, beneficiary interactions with the robot system, etc. We wanted to create a form of general intelligence where the robot would "know" how to bring the food item to the beneficiary, regardless of the situation it is being presented. Therefore, we wanted to try implementing a learning aspect to the project. By creating various scenarios, and using the design of the robot to dictate how the food is brought to the beneficiary, we hoped to enact reinforcement learning where the robot learns the intended action to carry out given the scenario, and develop a policy which enable the robot to autonomously create pathways and bring the food item to the beneficiary.</p>
  
              <p>We felt that the design choices we made fulfilled our design objectives in creating a simplified design which was efficient in implementation while being highly reproducible in executing what we wanted the system to do in serving the needs of the beneficiaries. We tackled the design philosophy on the constrained and dynamic front of varying levels of difficulty, while ensuring that user-friendliness remained our top priority, ensuring a created system that would be durable. We were unsure over how robust this system would be in executing the task in the real world, but felt that it is a good starting point to build off of.</p>
  
              <div style="display: flex; justify-content: center; margin: 40px 0;">
                <img src="./media/luis_end_user.png" 
                     alt="Luis - End User"
                     style="width: 50%; 
                            border-radius: 8px;
                            box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
              </div>
  
              <div style="display: flex; justify-content: center; margin: 40px 0;">
                <div id="schematic" style="width: 80%; height: 300px;"></div>
              </div>
  
            </div>
          </details>
        </div>
      </div>
    </div>
  </section>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.3.0/raphael.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.15.0/flowchart.min.js"></script>
  <script>
    var diagramCode = `
      st=>start: Using Teleoperation to create path trajectory
      op1=>operation: Enables human input, caretakers can dictate how the robot brings the food to the mouth of the beneficiary
      op2=>operation: Enabling Computer Vision to see beneficiary's face
      op3=>operation: Follower arm executes path trajectory for feeding when beneficiary opens mouth
      e=>end: Quadriplegic communities mainly communicate through facial expressions; this creates a communication bridge between robot system and them
  
      st->op1->op2->op3->e
    `;
    flowchart.parse(diagramCode).drawSVG('schematic', {
      'x': 0,
      'y': 0,
      'line-width': 2,
      'line-length': 50,
      'text-margin': 10,
      'font-size': 14,
      'font-color': 'black',
      'line-color': 'black',
      'element-color': 'black',
      'fill': 'white',
      'yes-text': 'yes',
      'no-text': 'no',
      'arrow-end': 'block',
      'scale': 1,
      'symbols': {
        'start': {
          'font-color': 'red',
          'element-color': 'red',
          'fill': 'yellow'
        },
        'end': {
          'class': 'end-element'
        }
      },
      'flowstate': {
        'past': { 'fill': '#CCCCCC', 'font-size': 12 },
        'current': { 'fill': 'yellow', 'font-color': 'red', 'font-weight': 'bold' },
        'future': { 'fill': '#FFFF99' },
        'request': { 'fill': 'blue' },
        'invalid': { 'fill': '#444444' },
        'approved': { 'fill': '#58C4A3', 'font-size': 12, 'yes-text': 'APPROVED', 'no-text': 'n/a' },
        'rejected': { 'fill': '#C45879', 'font-size': 12, 'yes-text': 'n/a', 'no-text': 'REJECTED' }
      }
    });
  </script>
  




<section class="hero teaser" style="margin-top: 20px;">
    <div class="container" style="margin-top: 20px;">
      <div class="hero-body" style="padding: 0; display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
        <figure style="width: 45%; margin: 0; margin-top: 20px;">
          <img src="./media/hardware_so_arm.gif" 
               alt="Hardware SO-ARM" 
               style="width: 100%;
               border-radius: 8px;
               display: block;
               margin: 0 auto;
               box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <figcaption style="text-align: center; margin-top: 10px;">
            Figure 3: Hardware demonstration of SO-ARM robot
          </figcaption>
        </figure>

        <figure style="width: 45%; margin: 0; margin-top: 20px;">
          <img src="./media/3d_printing.gif"
               alt="3D Printing Process" 
               style="width: 100%;
               border-radius: 8px;
               display: block;
               margin: 0 auto;
               box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <figcaption style="text-align: center; margin-top: 10px;">
            Figure 4: 3D printing process of robot components
          </figcaption>
        </figure>
      </div>
    </div>
  </section>




<section class="hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content">
        <details>
          <summary class="title is-3" style="cursor: pointer; margin-bottom: 1rem;">Building the Hardware</summary>
          <div class="content has-text-justified">
            <h3>Implementation</h3>
            <h4>Journal of Building the SOARM</h4>
            <p>We spent a considerable amount of time choosing a design for the robotic arm. We carefully reviewed and selected a few key constraints we wanted to apply to the robot:</p>
            <ul>
              <li><strong>Low cost and replicable:</strong> We wanted to choose an accessible design that could be easily reproducible. If we were to make a successful product, we would want to be able to deploy it easily.</li>
              <li><strong>Safe to use:</strong> Safety was a huge priority. The robots we are designing are going to act autonomously, operating near the user's face.</li>
              <li><strong>Easily adaptable:</strong> We wanted to ensure that our robot worked in various environments to avoid having a consistent setup necessary for the robot to function properly. Making it easy to maintain also became an important factor.</li>
            </ul>
            <h4>Selection and Building Process</h4>
            <p>Due to the time constraints of the class, we decided it was best to use an open source robot found online. We eventually settled on the SOARM-100, as it satisfied all of our requirements outlined above, and it fit the overall design philosophy we wanted to employ. The SOARM-100 uses a teleoperation setup that involves 2 arms, one where a human operator manually creates the intended path trajectories, and the other copying the created motion synchronously (TheRobotStudio et al., 2024).</p>
            <div style="display: flex; justify-content: center; gap: 20px; margin: 20px 0;">
                <img src="./media/follower_arm.png"
                     alt="follower arm"
                     style="width: 35%;
                            box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                <img src="./media/leader_arm.png"
                     alt="leader arm" 
                     style="width: 35%;
                            box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            </div>

            <p>The SO-ARM 100 uses STS3215 servos, which utilize metal gears and magnetic encoders. With the encoders in the servos and the lerobot linux environment codebase used to operate the SO-ARM 100, the joint states of angle of rotation of each servo over a period of time could be easily recorded, stored and replicated. This step was crucial to simulate real-world scenarios accurately and align the robot's behavior with the actions and intentions of the caregiver.</p>

            <h4>3D Printing and Assembly</h4>
            <p>We printed the SOARM-100 using PLA, which is a durable, accurate, and relatively cheap material. According to our calculations, both of the robots together (the follower and the leader) incur the total cost of: 511 grams of PLA * $0.025/gram * 2 robots = $25.55 total. The servos are slightly more expensive than a typical servo (roughly $30 each), but we agreed that the cost difference was worth both the increase in safety for the user, as well as the longevity of the robot overall.</p>

            <p>Properly tolerancing the prints quickly became the next challenge. The SOARM-100 is designed to have the STS3215 servos press fit into it, to ensure precision. All parts were printed at Jacobs Hall, with Prusa i3 Mk3 printers over a span of 72 hours, with a total print time of around 48 hours.</p>
            <img src="./media/all_hardware_components.png"
                 alt="All Hardware Components" 
                 style="width: 40%;
                        border-radius: 8px;
                        display: block;
                        margin: 0 auto;
                        box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            <h4>Software Integration</h4>
            <p>The STS3215 servos were connected via a daisy chain and configured with unique IDs. We utilized scripts from the official repository (Alibert et al., 2024) for:</p>
            <ul>
              <li>Finding port addresses for microcontroller-computer connection</li>
              <li>Configuring motor IDs</li>
              <li>Centering motors for accurate rotation ranges (-2048 to 2048)</li>
            </ul>

            <h4>Camera Integration</h4>
            <p>For the mouth-open detection system, we adapted an existing camera mount design to work with the Innomaker 720p USB2.0 UVC Camera, modifying it to fit the SOARM-100  (Chekuri, 2024). We learnt the need of putting a camera right next to the gripper to ensure that we could gather camera data of the gripper opening and closing. This enables visual feedback for the robot's operation and user interaction.</p>
            <img src="./media/camera_mount.png"
                 alt="Camera Mount Design"
                 style="width: 40%;
                        border-radius: 8px;
                        display: block;
                        margin: 0 auto;
                        box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
        </details>
      </div>
    </div>
  </div>
</section>





<section class="hero teaser">
  <div class="container">
    <div class="hero-body" style="padding: 0; display: flex; flex-direction: column; align-items: center;">
      <div style="display: flex; justify-content: center; gap: 20px;">


        <figure style="width: 25%; margin: 0;">
          <img src="./media/boris_teleop.gif" alt="Boris Teleoperation" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <figcaption style="text-align: center; margin-top: 5px;">Figure 4: Boris demonstrating teleoperation</figcaption>
        </figure>

        <figure style="width: 25%; margin: 0;">
            <img src="./media/data_visualization_01.gif" alt="data_visualization_01" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <figcaption style="text-align: center; margin-top: 5px;">Figure 5: Dataset visualization of pick-and-place task</figcaption>
        </figure>

        <figure style="width: 25%; margin: 0;">
          <img src="./media/francesco_teleoperating_2.gif" alt="Francesco Teleoperation" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <figcaption style="text-align: center; margin-top: 5px;">Figure 6: Francesco performing teleoperation</figcaption>
        </figure>

        <figure style="width: 25%; margin: 0;">
            <img src="./media/visualize_dataset_pick_and_place.gif" alt="Boris Teleoperation" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          <figcaption style="text-align: center; margin-top: 5px;">Figure 7: Data visualization of robot movements</figcaption>
        </figure>

      </div>
    </div>
  </div>
</section>


  
<section class="hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content">
        <details>
          <summary class="title is-3" style="cursor: pointer; margin-bottom: 1rem;">Teleoperation and Data Collection</summary>
          <div class="content has-text-justified">
            <p>To develop a robust and reliable robotic feeding assistant, we implemented a comprehensive data collection strategy utilizing our teleoperation setup. The system was designed to capture both the physical movements of the robot and visual feedback from multiple perspectives, enabling us to create a rich dataset for training and validation purposes.</p>

            <h4>Data Collection Environment</h4>
            <p>Our data collection setup consisted of two strategically placed cameras:</p>
            <ul>
              <li>An end-effector mounted camera providing a first-person perspective of the manipulation tasks</li>
              <li>An overhead camera positioned 1.5 meters above the workspace center, capturing the entire scene including the follower robot and the beneficiary</li>
            </ul>

            <p>This dual-camera configuration allowed us to simultaneously monitor the robot's precise movements and maintain a comprehensive view of the interaction space. The overhead camera proved particularly valuable for pick-and-place tasks, providing clear visibility of object positions and the target locations.</p>

            <h4>Data Recording and Processing</h4>
            <p>During each teleoperated session, we recorded:</p>
            <ul>
              <li>Joint states from the STS3215 servo encoders at each frame</li>
              <li>Synchronized video feeds at 30 FPS from both cameras</li>
              <li>Temporal alignment of joint positions with visual data</li>
            </ul>

            <p>The data collection process yielded approximately 300 episodes across various tasks, including pick-and-place operations and feeding motions with baby carrots. Each episode was automatically processed to calculate key statistical metrics, including mean trajectories and standard deviations, which were subsequently uploaded to Hugging Face for further analysis.</p>

            <h4>Data Validation and Refinement</h4>
            <p>To ensure data quality, we implemented a rigorous validation process:</p>
            <ul>
              <li>Visual inspection of each recorded episode</li>
              <li>Classification of episodes into successful and unsuccessful attempts</li>
              <li>Temporal trimming to isolate relevant motion segments</li>
              <li>Verification through physical replay on the original setup</li>
            </ul>

            <p>The replay verification step was particularly crucial, as it allowed us to confirm the accuracy of our recorded trajectories and ensure that the robot could faithfully reproduce the intended motions. This double-validation approach significantly enhanced the reliability of our dataset.</p>

            <h4>Custom Scripts and Implementation</h4>
            <p>The elalcance contains five files in total, the .pycache and __init__.py files allow for functions created within this folder to be exported and executable. The mouth_recognition.py file provides a window for a live demonstration of the code used for implementing the mesh for mouth-open detection, and mouth_recog.py turns the code into a exportable function named mouth_open_activate which returns a boolean value, only giving a True output when it detects that a mouth being open. Otherwise, when the mouth remains closed the program continues to run.</p>

            <figure>
              <img src="./media/script.png" alt="Custom Scripts Structure" style="width: 70%; border-radius: 8px; display: block; margin: 0 auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
              <figcaption style="text-align: center; margin-top: 10px;">Custom scripts structure and implementation</figcaption>
            </figure>

            <p>(Please refer to our <a href="https://github.com/Francescocrivelli/le-alcance" target="_blank">README.md</a> for more information on the codebase) Through this systematic approach to data collection and validation, we established a comprehensive dataset that effectively captures the nuances of human-guided robotic manipulation. This data forms the foundation for developing robust path-planning algorithms and ensuring consistent, reliable performance in assistive feeding scenarios.</p>
          </div>
        </details>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
    <div class="container">
        <div class="hero-body" style="padding: 0; display: flex; flex-direction: column; align-items: center;">
            <div style="display: flex; justify-content: center; gap: 20px;">
                <figure style="width: 25%; margin: 0;">
                    <img src="./media/mouth_detection_on_bryan.gif" alt="Mouth Detection on Bryan" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <figcaption style="text-align: center; margin-top: 5px;">Figure 8: Mouth detection on Bryan</figcaption>
                </figure>

                <figure style="width: 25%; margin: 0;">
                    <img src="./media/francesco_eating_with hand.gif" alt="Francesco Eating with Hand" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <figcaption style="text-align: center; margin-top: 5px;">Figure 9: Francesco eating with hand</figcaption>
                </figure>

                <figure style="width: 25%; margin: 0;">
                    <img src="./media/boris_eating.gif" alt="Boris Eating" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <figcaption style="text-align: center; margin-top: 5px;">Figure 10: Data collection</figcaption>
                </figure>

                <figure style="width: 25%; margin: 0;">
                    <img src="./media/robot_fpv_feed_francesco.gif" alt="Robot FPV Feeding Francesco" style="width: 100%; aspect-ratio: 1; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
                    <figcaption style="text-align: center; margin-top: 5px;">Figure 11: Robot FPV feeding Francesco</figcaption>
                </figure>
            </div>
        </div>
    </div>
</section>




<section class="hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content">
        <details>
          <summary class="title is-3" style="cursor: pointer; margin-bottom: 1rem;">Training ACT Policy and Results</summary>
          <div class="content has-text-justified">
            <h4>Training Methodology</h4>
            <p>We implemented the Affordance-based Conditioning for Transformer (ACT) policy training using a comprehensive dataset collected from our teleoperation setup. The training process was conducted using Google Colab's computational resources, although the extensive nature of the transformer architecture resulted in significant training durations. Our primary focus was on the pick-and-place task, for which we had collected robust demonstration data.</p>

            <p>The training pipeline incorporated several key components:</p>
            <ul>
              <li>Temporal sequence processing of joint states and visual features</li>
              <li>Integration of multi-modal inputs (proprioceptive and visual)</li>
              <li>Transformer-based attention mechanisms for trajectory prediction</li>
              <li>Cross-validation using held-out demonstration episodes</li>
            </ul>

            <h4>Low-Level Policy Language Conditioning</h4>
            <p>Our approach to policy conditioning followed a structured data collection template:</p>
            <ul>
              <li>Baseline dataset: 100 episodes of standard feeding interactions</li>
              <li>Positional variations: 50 episodes from alternative feeding positions</li>
              <li>Object variations: 50 episodes per different food item</li>
            </ul>

            <p>The language conditioning was implemented through semantic signal association. For instance, we collected 50 episodes with the signal "feed carrot" and another 50 with "feed cherry," enabling the model to develop object-specific manipulation strategies. This methodology creates a semantic mapping between natural language commands and corresponding manipulation behaviors.</p>

            <h4>Visual Activation System</h4>
            <p>Based on insights from interviews with beneficiaries and their families at the non-profit organization, we identified mouth opening as a critical signal for feeding initiation. Our system architecture comprises sophisticated computer vision components working in concert with robotic control systems.</p>

            <p>The technical implementation includes:</p>
            <ul>
              <li>Video Processing Pipeline:
                <ul>
                  <li>MediaPipe libraries integration for real-time face detection</li>
                  <li>OpenCV-based FaceMesh implementation for facial point tracking</li>
                  <li>Euclidean distance computation between upper and lower lip coordinates</li>
                  <li>Confidence threshold validation for activation triggers</li>
                </ul>
              </li>
              <li>Hardware Configuration:
                <ul>
                  <li>Logitech C922 webcam (30 FPS) for live demonstrations</li>
                  <li>Innomaker 720p USB2.0 UVC Camera for video recordings</li>
                  <li>Jetson Nano processor for computational tasks</li>
                </ul>
              </li>
              <li>Safety Features:
                <ul>
                  <li>Confidence threshold requirements for activation</li>
                  <li>Continuous monitoring and validation of facial landmarks</li>
                  <li>Real-time verification of mouth state detection</li>
                </ul>
              </li>
            </ul>

            <h4>Training Results and Analysis</h4>
            <p>The training process yielded interesting insights into the challenges of robotic manipulation learning:</p>
            <ul>
              <li>Loss curves exhibited notable volatility, particularly in the pick-and-place task</li>
              <li>Trajectory variations in velocity and approach angles contributed to training noise</li>
              <li>Dataset heterogeneity impacted convergence characteristics</li>
            </ul>

            <img src="./media/training_loss.png"
                 alt="Training Loss Plot"
                 style="width: 60%;
                        border-radius: 8px;
                        display: block;
                        margin: 20px auto;
                        box-shadow: 0 4px 8px rgba(0,0,0,0.1);">

            <p>The observed noise in the loss function can be attributed to several factors:</p>
            <ul>
              <li>Diverse demonstration velocities in the training dataset</li>
              <li>Varying approach angles during pick-and-place operations</li>
              <li>Natural variability in human teleoperation styles</li>
              <li>Multi-modal learning challenges across visual and proprioceptive domains</li>
            </ul>

            <p>To ensure the reliability of our trained policies, we conducted extensive validation through physical replay on the original setup, verifying the model's ability to generalize across different scenarios while maintaining safety constraints.</p>
          </div>
        </details>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container">
    <div class="hero-body" style="padding: 0; display: flex; justify-content: center; gap: 20px;">
      <figure style="width: 45%; margin: 0;">
        <video width="100%" autoplay loop muted playsinline style="border-radius: 12px; box-shadow: 0 6px 12px rgba(0,0,0,0.15); object-fit: cover;">
          <source src="./media/full_demo_official.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <figcaption style="text-align: center; margin-top: 10px;">
          Figure 8: Full system demonstration
        </figcaption>
      </figure>

      <figure style="width: 45%; margin: 0;">
        <video width="100%" autoplay loop muted playsinline style="border-radius: 12px; box-shadow: 0 6px 12px rgba(0,0,0,0.15); object-fit: cover;">
          <source src="./media/bryan_being_fed.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <figcaption style="text-align: center; margin-top: 10px;">
          Figure 9: Bryan testing the feeding system
        </figcaption>
      </figure>
    </div>
  </div>
</section>

<p>
    
</p>

<!-- 
<section class="hero teaser">
  <div class="container">
    <div class="hero-body" style="padding: 0;">
      <img src="./static/images/rotation_3000_steps.gif" 
           alt="NeRF Rotation" 
           style="width: 100vw; 
                  max-width: 700px;
                  height: auto;
                  margin: 0 auto;
                  display: block;
                  border-radius: 12px;
                  box-shadow: 0 6px 12px rgba(0,0,0,0.2);">
    </div>
  </div>
</section> -->



<section class="hero" style="display: flex; justify-content: center;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content">
        <details>
          <summary class="title is-3" style="cursor: pointer; margin-bottom: 1rem;">Conclusion and System Analysis</summary>
          <div class="content has-text-justified">
            <h3>System Performance and Design Criteria Evaluation</h3>
            
            <p>Our implemented system successfully demonstrated the core functionality of trajectory storage and replication, activated through a computer vision-based mouth-opening detection system. The current implementation achieves single-trajectory actuation per mouth-open event, though further development is required to enable multiple trajectory replications across repeated activations.</p>

            <h3>Technical Challenges and Implementation Constraints</h3>

            <p>A significant technical challenge emerged in the end-effector design phase. Initial attempts focused on developing CAD-designed spoon holder attachments for the SO-ARM100's end effector, with the intention of enabling food scooping and delivery operations. However, practical testing revealed considerable limitations in this approach:</p>

            <ul>
              <li>Inconsistent gripper-to-holder attachment mechanics</li>
              <li>Unstable motion patterns post-attachment</li>
              <li>Limited precision in food manipulation tasks</li>
            </ul>

            <div style="display: flex; justify-content: center; margin: 20px 0;">
              <img src="./media/spoon.png" 
                   alt="Spoon holder prototype design"
                   style="width: 50%; 
                          border-radius: 8px;
                          box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            </div>

            <h3>Future Development Pathways</h3>

            <p>Given resource and time constraints, we pivoted to direct food manipulation using the existing gripper system, focusing on handling discrete food items such as carrots or broccoli. While this approach demonstrated system viability, our user research indicates that spoon-based feeding remains the preferred method for beneficiaries. Future iterations should prioritize:</p>

            <ul>
              <li>Comprehensive redesign of the SO-ARM100 end-effector</li>
              <li>Integration of specialized spoon-holding mechanisms</li>
              <li>Software optimization for fluid, spoon-based feeding motions</li>
              <li>Enhanced trajectory replication capabilities</li>
            </ul>

            <p>These improvements would align the system more closely with established feeding practices while maintaining the core benefits of our current implementation.</p>
          </div>
        </details>
      </div>
    </div>
  </div>
</section>


<section class="hero">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content">
        <details>
          <summary class="title is-3" style="cursor: pointer; margin-bottom: 1rem;">Team</summary>
          <div class="content has-text-justified">
            <div style="display: flex; justify-content: center; margin: 20px 0;">
              <img src="./media/team_image.png" 
                   alt="Team Photo"
                   style="width: 100%; 
                          border-radius: 8px;
                          box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            </div>
            <p style="text-align: center; font-style: italic; margin-top: -10px;">
              The picture above was taken on Tuesday RRR week at 5:30 am
            </p>
          </div>
          </div>
        </details>
      </div>
    </div>
  </div>
</section>



<!-- MathJax for LaTeX rendering -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


<section class="hero teaser">
  <div class="container">
    <div class="hero-body" style="padding: 0;">
      <h2 class="title has-text-centered" style="margin: 60px 0 20px 0;">
        Final Project Video
      </h2>
      <figure style="margin: 40px 0;">
        <iframe width="560" height="315" 
                src="https://www.youtube.com/embed/DRsAnFrGtw4?autoplay=1&mute=1"
                title="YouTube video player" 
                frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen
                style="border-radius: 8px; display: block; margin: 0 auto; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
        </iframe>
      </figure>
    </div>
  </div>
</section>









<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">AL-CANCE</span> High-level robotic feeding assistant
      </h2>
    </div>
  </div>
</section>






<section class="section" id="BibTeX"></section>
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>

    <div class="content has-text-justified">
      <p>
        <strong>[1]</strong> TheRobotStudio, Moss, J., Cadene, R., & Alibert, S. (2024). TheRobotStudio/so-ARM100: Standard open arm 100. SO-ARM100. 
        <a href="https://github.com/TheRobotStudio/SO-ARM100?tab=readme-ov-file">https://github.com/TheRobotStudio/SO-ARM100</a>
      </p>

      <p>
        <strong>[2]</strong> Alibert, S., Cadene, R., Soare, A., Gallouedec, Q., Zouitine, A., & Wolf, T. (2024). Lerobot/examples/10_use_so100.md at main · Huggingface/lerobot. LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch.
        <a href="https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md">https://github.com/huggingface/lerobot</a>
      </p>

      <p>
        <strong>[3]</strong> Chekuri, S. R. (2024). Meetsitaram/koch-V1-1: A version 1.1 of the alexander koch low cost robot arm with some small changes. Low-Cost Robot Arm: Koch v1.1.
        <a href="https://github.com/meetsitaram/koch-v1-1/blob/camera_mounts/hardware/follower/STL/Gripper_Cam_Mount.stl">https://github.com/meetsitaram/koch-v1-1</a>
      </p>
      <h2>cite our work</h2>
      <pre><code>@misc{crivelli2024lealcance,
        author = {Francesco C., Bryan S., Claire B., and Boris},
        title = {Le-Alcance: A Fork of LeRobot for Low-Cost Robotic Learning},
        howpublished = "\url{https://github.com/francescocrivelli/le-alcance}",
        year = {2024}
        }</code></pre>
    </div>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is borrowed from <a href="https://nerfies.github.io/">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
